# big_data_challenge

![png-clipart-big-data-data-analysis-information-data-processing-data-lake-big-data-text-logo-thumbnail](https://user-images.githubusercontent.com/58186835/204578442-50157d6b-b72b-457e-af17-67025170229c.png)

## Objectives
1. Import PySpark and other dependencies
1. Create PySpark session
1. Identify two datasets from Amazon
1. Retrieve the data from S3 buckets
1. Create PySpark data frames
1. Set up PostgreSQL database on AWS/RDS
1. Set up local PostgreSQL database
1. Replicate the schema from the dataframes and create SQL tables
