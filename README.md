# big_data_challenge

![png-clipart-big-data-data-analysis-information-data-processing-data-lake-big-data-text-logo-thumbnail](https://user-images.githubusercontent.com/58186835/204578442-50157d6b-b72b-457e-af17-67025170229c.png)

## Objectives
1. Import PySpark and other dependencies
1. Create PySpark session
1. Identify two datasets from Amazon
1. Retrieve the data from S3 buckets
1. Create PySpark data frames
1. Set up PostgreSQL database on AWS/RDS
1. Set up local PostgreSQL database
1. Replicate the schema from the dataframes and create SQL tables

## Observations
1. PySpark is an efficient way to handle large data sets.
1. As somneone who works with data all day long, I appreciate the power of PySpark.
1. In combination with dataframes, one could do a lot of ETL work in this environment.

### Note
#### I did not opt to do level-2 work. I am recovering from knee surgery and not to put too fine a point on it, I'm glad to get level-1 turned in.

# Thank you graders for your efforts!
